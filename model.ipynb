{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f884df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ikomia.dataprocess.workflow import Workflow\n",
    "from ikomia.utils.displayIO import display\n",
    "from ikomia.utils import ik\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\torchreid\\reid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (c:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (c:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (c:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (c:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (c:\\Users\\BluSerK\\anaconda3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "# Replace 'your_video_path.mp4' with the actual video file path\n",
    "input_video_path = 'walking.mp4'\n",
    "output_video_path = 'deepsort_output_video.avi'\n",
    "\n",
    "# Init your workflow\n",
    "wf = Workflow()\n",
    "\n",
    "# Add object detection algorithm\n",
    "detector = wf.add_task(ik.infer_yolo_v11(), auto_connect=True)\n",
    "detector.set_parameters({\"cuda\": \"True\"})\n",
    "# Add ByteTrack tracking algorithm\n",
    "tracking = wf.add_task(name=\"infer_deepsort\", auto_connect=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking.set_parameters({\n",
    "    \"categories\": \"all\",\n",
    "    \"conf_thres\": \"0.5\",\n",
    "    \"cuda\": \"True\"\n",
    "})\n",
    "\n",
    "# Open the video file\n",
    "stream = cv2.VideoCapture(input_video_path)\n",
    "if not stream.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20dd88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video properties for the output\n",
    "frame_width = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = stream.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "# The 'XVID' codec is widely supported and provides good quality\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b9529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x640 9 persons, 1 backpack, 2 handbags, 119.2ms\n",
      "Speed: 4.6ms preprocess, 119.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 552.8437 ms.\n",
      "\n",
      "0: 640x640 9 persons, 1 backpack, 2 handbags, 105.2ms\n",
      "Speed: 3.8ms preprocess, 105.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 601.4546 ms.\n",
      "\n",
      "0: 640x640 9 persons, 1 backpack, 2 handbags, 108.7ms\n",
      "Speed: 3.9ms preprocess, 108.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 600.8689 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 1 handbag, 89.3ms\n",
      "Speed: 2.9ms preprocess, 89.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 636.4208 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 backpack, 1 handbag, 97.1ms\n",
      "Speed: 3.1ms preprocess, 97.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 559.3838999999999 ms.\n",
      "\n",
      "0: 640x640 9 persons, 1 backpack, 92.7ms\n",
      "Speed: 3.5ms preprocess, 92.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 482.4875 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 86.1ms\n",
      "Speed: 3.2ms preprocess, 86.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 506.1352 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 76.1ms\n",
      "Speed: 2.7ms preprocess, 76.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 481.51349999999996 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 backpack, 66.5ms\n",
      "Speed: 3.1ms preprocess, 66.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 471.0289 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 backpack, 84.5ms\n",
      "Speed: 2.6ms preprocess, 84.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 503.3005 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 backpack, 1 handbag, 68.3ms\n",
      "Speed: 3.1ms preprocess, 68.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 470.10249999999996 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 backpack, 68.3ms\n",
      "Speed: 3.2ms preprocess, 68.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 431.4122 ms.\n",
      "\n",
      "0: 640x640 9 persons, 1 backpack, 67.4ms\n",
      "Speed: 3.5ms preprocess, 67.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 401.18539999999996 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 71.5ms\n",
      "Speed: 3.2ms preprocess, 71.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 378.5032 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 67.3ms\n",
      "Speed: 3.3ms preprocess, 67.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 396.5255 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 89.4ms\n",
      "Speed: 3.0ms preprocess, 89.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 406.4584 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 67.7ms\n",
      "Speed: 2.7ms preprocess, 67.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 378.8922 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 67.9ms\n",
      "Speed: 2.8ms preprocess, 67.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 368.8493 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 68.3ms\n",
      "Speed: 3.3ms preprocess, 68.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 400.017 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 85.1ms\n",
      "Speed: 3.2ms preprocess, 85.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 397.73740000000004 ms.\n",
      "\n",
      "0: 640x640 8 persons, 1 backpack, 69.0ms\n",
      "Speed: 3.4ms preprocess, 69.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 387.701 ms.\n",
      "\n",
      "0: 640x640 8 persons, 79.5ms\n",
      "Speed: 3.1ms preprocess, 79.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 392.4922 ms.\n",
      "\n",
      "0: 640x640 9 persons, 72.8ms\n",
      "Speed: 3.1ms preprocess, 72.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 366.2794 ms.\n",
      "\n",
      "0: 640x640 8 persons, 65.6ms\n",
      "Speed: 3.3ms preprocess, 65.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 325.5783 ms.\n",
      "\n",
      "0: 640x640 10 persons, 78.8ms\n",
      "Speed: 3.3ms preprocess, 78.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 414.94179999999994 ms.\n",
      "\n",
      "0: 640x640 11 persons, 65.8ms\n",
      "Speed: 3.2ms preprocess, 65.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 430.7743 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 65.7ms\n",
      "Speed: 3.2ms preprocess, 65.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 462.49899999999997 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 82.3ms\n",
      "Speed: 3.1ms preprocess, 82.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 523.064 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 72.8ms\n",
      "Speed: 3.0ms preprocess, 72.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 493.2715 ms.\n",
      "\n",
      "0: 640x640 12 persons, 1 backpack, 67.5ms\n",
      "Speed: 2.7ms preprocess, 67.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 487.22529999999995 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 1 skateboard, 69.3ms\n",
      "Speed: 3.3ms preprocess, 69.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 480.7554 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 1 skateboard, 69.8ms\n",
      "Speed: 3.1ms preprocess, 69.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 495.0039 ms.\n",
      "\n",
      "0: 640x640 11 persons, 68.3ms\n",
      "Speed: 3.0ms preprocess, 68.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 461.9295 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 69.5ms\n",
      "Speed: 3.1ms preprocess, 69.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 454.5083 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 backpack, 67.0ms\n",
      "Speed: 3.2ms preprocess, 67.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 423.3068 ms.\n",
      "\n",
      "0: 640x640 9 persons, 64.5ms\n",
      "Speed: 3.4ms preprocess, 64.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 350.0599 ms.\n",
      "\n",
      "0: 640x640 9 persons, 63.0ms\n",
      "Speed: 3.3ms preprocess, 63.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 361.5781 ms.\n",
      "\n",
      "0: 640x640 9 persons, 71.4ms\n",
      "Speed: 3.3ms preprocess, 71.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 359.6096 ms.\n",
      "\n",
      "0: 640x640 8 persons, 70.9ms\n",
      "Speed: 3.1ms preprocess, 70.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 351.0696 ms.\n",
      "\n",
      "0: 640x640 10 persons, 77.4ms\n",
      "Speed: 2.7ms preprocess, 77.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 443.5722 ms.\n",
      "\n",
      "0: 640x640 10 persons, 71.8ms\n",
      "Speed: 3.5ms preprocess, 71.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 419.5801 ms.\n",
      "\n",
      "0: 640x640 11 persons, 86.3ms\n",
      "Speed: 3.3ms preprocess, 86.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 514.9507 ms.\n",
      "\n",
      "0: 640x640 11 persons, 80.2ms\n",
      "Speed: 3.5ms preprocess, 80.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 506.9744 ms.\n",
      "\n",
      "0: 640x640 10 persons, 82.5ms\n",
      "Speed: 3.0ms preprocess, 82.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 470.4296 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 backpack, 84.5ms\n",
      "Speed: 2.8ms preprocess, 84.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 501.201 ms.\n",
      "\n",
      "0: 640x640 10 persons, 84.8ms\n",
      "Speed: 3.2ms preprocess, 84.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 413.7403 ms.\n",
      "\n",
      "0: 640x640 11 persons, 96.9ms\n",
      "Speed: 2.9ms preprocess, 96.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 460.8542 ms.\n",
      "\n",
      "0: 640x640 12 persons, 66.9ms\n",
      "Speed: 2.8ms preprocess, 66.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 460.9653 ms.\n",
      "\n",
      "0: 640x640 12 persons, 1 handbag, 82.0ms\n",
      "Speed: 3.1ms preprocess, 82.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 491.7322 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 1 handbag, 69.4ms\n",
      "Speed: 3.1ms preprocess, 69.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 523.4549 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 1 handbag, 62.7ms\n",
      "Speed: 3.4ms preprocess, 62.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 471.3844 ms.\n",
      "\n",
      "0: 640x640 12 persons, 1 backpack, 66.4ms\n",
      "Speed: 3.4ms preprocess, 66.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 479.25570000000005 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 1 handbag, 67.4ms\n",
      "Speed: 3.8ms preprocess, 67.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 488.0111 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 backpack, 1 handbag, 70.3ms\n",
      "Speed: 3.0ms preprocess, 70.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 485.3561 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 handbag, 65.7ms\n",
      "Speed: 3.2ms preprocess, 65.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 475.6976 ms.\n",
      "\n",
      "0: 640x640 11 persons, 1 handbag, 82.7ms\n",
      "Speed: 3.0ms preprocess, 82.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 491.4366 ms.\n",
      "\n",
      "0: 640x640 13 persons, 63.9ms\n",
      "Speed: 3.2ms preprocess, 63.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 561.4695 ms.\n",
      "\n",
      "0: 640x640 12 persons, 63.7ms\n",
      "Speed: 3.8ms preprocess, 63.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 508.37219999999996 ms.\n",
      "\n",
      "0: 640x640 13 persons, 1 handbag, 78.1ms\n",
      "Speed: 3.2ms preprocess, 78.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 617.2928999999999 ms.\n",
      "\n",
      "0: 640x640 12 persons, 90.2ms\n",
      "Speed: 3.0ms preprocess, 90.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 546.6975 ms.\n",
      "\n",
      "0: 640x640 12 persons, 1 handbag, 1 surfboard, 85.0ms\n",
      "Speed: 3.0ms preprocess, 85.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 526.8554 ms.\n",
      "\n",
      "0: 640x640 12 persons, 79.4ms\n",
      "Speed: 3.1ms preprocess, 79.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 467.3605 ms.\n",
      "\n",
      "0: 640x640 13 persons, 74.3ms\n",
      "Speed: 3.3ms preprocess, 74.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 634.803 ms.\n",
      "\n",
      "0: 640x640 13 persons, 103.1ms\n",
      "Speed: 3.7ms preprocess, 103.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 815.239 ms.\n",
      "\n",
      "0: 640x640 12 persons, 77.5ms\n",
      "Speed: 6.0ms preprocess, 77.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 637.2894 ms.\n",
      "\n",
      "0: 640x640 11 persons, 114.9ms\n",
      "Speed: 5.4ms preprocess, 114.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 956.2762 ms.\n",
      "\n",
      "0: 640x640 10 persons, 142.9ms\n",
      "Speed: 8.6ms preprocess, 142.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 870.4516 ms.\n",
      "\n",
      "0: 640x640 10 persons, 117.4ms\n",
      "Speed: 4.0ms preprocess, 117.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 907.7261000000001 ms.\n",
      "\n",
      "0: 640x640 13 persons, 163.9ms\n",
      "Speed: 5.6ms preprocess, 163.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 1285.4687000000001 ms.\n",
      "\n",
      "0: 640x640 11 persons, 382.5ms\n",
      "Speed: 5.7ms preprocess, 382.5ms inference, 22.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 1792.2239 ms.\n",
      "\n",
      "0: 640x640 11 persons, 195.7ms\n",
      "Speed: 4.7ms preprocess, 195.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 894.0310000000001 ms.\n",
      "\n",
      "0: 640x640 12 persons, 178.0ms\n",
      "Speed: 6.6ms preprocess, 178.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 1228.2907 ms.\n",
      "\n",
      "0: 640x640 10 persons, 145.2ms\n",
      "Speed: 4.5ms preprocess, 145.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 849.2601999999999 ms.\n",
      "\n",
      "0: 640x640 13 persons, 143.1ms\n",
      "Speed: 6.4ms preprocess, 143.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 996.5564 ms.\n",
      "\n",
      "0: 640x640 14 persons, 228.6ms\n",
      "Speed: 4.5ms preprocess, 228.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 2163.9933 ms.\n",
      "\n",
      "0: 640x640 13 persons, 1 backpack, 192.7ms\n",
      "Speed: 3.5ms preprocess, 192.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 1264.1725999999999 ms.\n",
      "\n",
      "0: 640x640 11 persons, 222.8ms\n",
      "Speed: 3.7ms preprocess, 222.8ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 1227.8043 ms.\n",
      "\n",
      "0: 640x640 9 persons, 186.2ms\n",
      "Speed: 18.1ms preprocess, 186.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 913.2221 ms.\n",
      "\n",
      "0: 640x640 10 persons, 181.0ms\n",
      "Speed: 4.2ms preprocess, 181.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 916.5884000000001 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 umbrella, 228.4ms\n",
      "Speed: 3.8ms preprocess, 228.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 985.6548 ms.\n",
      "\n",
      "0: 640x640 10 persons, 572.8ms\n",
      "Speed: 4.5ms preprocess, 572.8ms inference, 15.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 1249.1318 ms.\n",
      "\n",
      "0: 640x640 10 persons, 170.6ms\n",
      "Speed: 5.3ms preprocess, 170.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 880.7415000000001 ms.\n",
      "\n",
      "0: 640x640 10 persons, 220.5ms\n",
      "Speed: 3.9ms preprocess, 220.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 878.2778 ms.\n",
      "\n",
      "0: 640x640 10 persons, 224.1ms\n",
      "Speed: 21.8ms preprocess, 224.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 862.8664 ms.\n",
      "\n",
      "0: 640x640 11 persons, 128.8ms\n",
      "Speed: 5.0ms preprocess, 128.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 717.9512000000001 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 umbrella, 1 handbag, 167.4ms\n",
      "Speed: 4.0ms preprocess, 167.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 736.3875 ms.\n",
      "\n",
      "0: 640x640 10 persons, 1 umbrella, 1 handbag, 111.0ms\n",
      "Speed: 3.6ms preprocess, 111.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 664.0618 ms.\n",
      "\n",
      "0: 640x640 12 persons, 1 umbrella, 1 handbag, 114.7ms\n",
      "Speed: 4.4ms preprocess, 114.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Workflow Untitled run successfully in 752.5367 ms.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Read image from stream\n",
    "    ret, frame = stream.read()\n",
    "\n",
    "    # Test if the video has ended or there is an error\n",
    "    if not ret:\n",
    "        print(\"Info: End of video or error.\")\n",
    "        break\n",
    "\n",
    "    # Run the workflow on current frame\n",
    "    wf.run_on(array=frame)\n",
    "\n",
    "    # Get results\n",
    "    image_out = tracking.get_output(0)\n",
    "    obj_detect_out = tracking.get_output(1)\n",
    "\n",
    "    # Convert the result to BGR color space for displaying\n",
    "    img_out = image_out.get_image_with_graphics(obj_detect_out)\n",
    "    img_res = cv2.cvtColor(img_out, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Save the resulting frame\n",
    "    out.write(img_out)\n",
    "\n",
    "    # Display\n",
    "    display(img_res, title=\"DeepSORT\", viewer=\"opencv\")\n",
    "\n",
    "    # Press 'q' to quit the video processing\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# After the loop release everything\n",
    "stream.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f66b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
